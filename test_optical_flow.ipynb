{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('/home/ubuntu/Workspace/utils/FishFeeding')\n",
    "import cv2\n",
    "from utils.video_preprocess import resize_video\n",
    "import numpy as np\n",
    "import mmcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - 预处理\n",
    "\n",
    "需要在视觉上基本克服常规水面波动的影响，例如吹风。\n",
    "\n",
    "### 1.1 - 预处理1：均值漂移"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('/Users/liuziyi/Desktop/220731_170753__220731_173752_clip_2.mp4')\n",
    "\n",
    "while(True):\n",
    "    check, img = cap.read()\n",
    "    if check:\n",
    "        cv2.imshow(\"raw\", img)\n",
    "\n",
    "        img_float = np.float32(img)  # Convert image from unsigned 8 bit to 32 bit float\n",
    "        criteria = (cv2.TERM_CRITERIA_EPS+cv2.TERM_CRITERIA_MAX_ITER, 10, 1)\n",
    "        meanshift = cv2.pyrMeanShiftFiltering(img, sp=8, sr=16, maxLevel=1, termcrit=(cv2.TERM_CRITERIA_EPS+cv2.TERM_CRITERIA_MAX_ITER, 5, 1))\n",
    "        # Apply meanshift algorithm on to image\n",
    "        cv2.imshow(\"Output of meanshift\", meanshift)\n",
    "        \n",
    "        key = cv2.waitKey(30)\n",
    "        if key >= 0:\n",
    "            cv2.waitKey(0)\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Close the window / Release webcam\n",
    "cap.release()\n",
    "  \n",
    "# De-allocate any associated memory usage \n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - 光流估计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmflow.apis import inference_model, init_model\n",
    "\n",
    "try:\n",
    "    import imageio\n",
    "except ImportError:\n",
    "    imageio = None\n",
    "\n",
    "# build the model from a config file and a checkpoint file\n",
    "config_file = '/Users/liuziyi/Workspace/mmsegmentation/configs/pspnet/pspnet_r101b-d8_512x1024_80k_cityscapes.py'\n",
    "config = mmcv.Config.fromfile(config_file)\n",
    "if device != 'cuda:0':\n",
    "    # modify as BN\n",
    "    pass\n",
    "checkpoint_file = '/Users/liuziyi/Workspace/FishFeeding/pspnet_r101b-d8_512x1024_80k_cityscapes_20201226_170012-3a4d38ab.pth'\n",
    "model = init_model(config, checkpoint_file, device=device)\n",
    "\n",
    "# load video\n",
    "video = '/home/ubuntu/Workspace/mmflow/demo/220731_170753__220731_173752_clip.mp4'\n",
    "cap = cv2.VideoCapture(video)\n",
    "size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "r = []\n",
    "while (cap.isOpened()):\n",
    "    # Get frames\n",
    "    flag1, img1 = cap.read()\n",
    "    flag2, img2 = cap.read()\n",
    "\n",
    "    if flag1 and flag2:\n",
    "        img1 = np.float32(img1)  # Convert image from unsigned 8 bit to 32 bit float\n",
    "        criteria = (cv2.TERM_CRITERIA_EPS+cv2.TERM_CRITERIA_MAX_ITER, 10, 1)\n",
    "        img1 = cv2.pyrMeanShiftFiltering(img1, sp=8, sr=16, maxLevel=1, termcrit=(cv2.TERM_CRITERIA_EPS+cv2.TERM_CRITERIA_MAX_ITER, 5, 1))\n",
    "\n",
    "        img2 = np.float32(img2)  # Convert image from unsigned 8 bit to 32 bit float\n",
    "        criteria = (cv2.TERM_CRITERIA_EPS+cv2.TERM_CRITERIA_MAX_ITER, 10, 1)\n",
    "        img2 = cv2.pyrMeanShiftFiltering(img2, sp=8, sr=16, maxLevel=1, termcrit=(cv2.TERM_CRITERIA_EPS+cv2.TERM_CRITERIA_MAX_ITER, 5, 1))\n",
    "\n",
    "        result = inference_model(model, img1, img2)\n",
    "        r.append(np.sum(np.sqrt(np.power(result[:, :, 0], 2) + np.power(result[:, :, 1], 2))))\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[7153]: Class CaptureDelegate is implemented in both /Users/liuziyi/opt/anaconda3/envs/openmmlab/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x17d056538) and /Users/liuziyi/opt/anaconda3/envs/openmmlab/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x123030860). One of the two will be used. Which one is undefined.\n",
      "objc[7153]: Class CVWindow is implemented in both /Users/liuziyi/opt/anaconda3/envs/openmmlab/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x17d056588) and /Users/liuziyi/opt/anaconda3/envs/openmmlab/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x117cd4a68). One of the two will be used. Which one is undefined.\n",
      "objc[7153]: Class CVView is implemented in both /Users/liuziyi/opt/anaconda3/envs/openmmlab/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x17d0565b0) and /Users/liuziyi/opt/anaconda3/envs/openmmlab/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x117cd4a90). One of the two will be used. Which one is undefined.\n",
      "objc[7153]: Class CVSlider is implemented in both /Users/liuziyi/opt/anaconda3/envs/openmmlab/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x17d0565d8) and /Users/liuziyi/opt/anaconda3/envs/openmmlab/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x117cd4ab8). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from cvzone.SelfiSegmentationModule import SelfiSegmentation\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture('/Users/liuziyi/Desktop/220731_170753__220731_173752_clip_2.mp4')\n",
    "\n",
    "while(True):\n",
    "    check, img = cap.read()\n",
    "    if check:\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "        # lab = cv2.cvtColor(img, cv2.COLOR_BGR2 )\n",
    "        us = yuv[:, :, 1] - hsv[:, :, 1]\n",
    "        \n",
    "        # cv2.imshow(\"u - s\", us)\n",
    "        cv2.imshow(\"raw\", img)\n",
    "        # cv2.imshow(\"hsv\", hsv)\n",
    "        # cv2.imshow(\"yuv\", yuv)\n",
    "\n",
    "        img_float = np.float32(img)  # Convert image from unsigned 8 bit to 32 bit float\n",
    "        criteria = (cv2.TERM_CRITERIA_EPS+cv2.TERM_CRITERIA_MAX_ITER, 10, 1)\n",
    "        # Defining the criteria ( type, max_iter, epsilon )\n",
    "        # cv2.TERM_CRITERIA_EPS - stop the algorithm iteration if specified accuracy, epsilon, is reached.\n",
    "        # cv2.TERM_CRITERIA_MAX_ITER - stop the algorithm after the specified number of iterations, max_iter.\n",
    "        # cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER - stop the iteration when any of the above condition is met.\n",
    "        # max_iter - An integer specifying maximum number of iterations.In this case it is 10\n",
    "        # epsilon - Required accuracy.In this case it is 1\n",
    "        # k = 50  # Number of clusters\n",
    "        # ret, label, centers = cv2.kmeans(img_float, k, None, criteria, 50, cv2.KMEANS_RANDOM_CENTERS)\n",
    "        # # apply kmeans algorithm with random centers approach\n",
    "        # center = np.uint8(centers)\n",
    "        # # Convert the image from float to unsigned integer\n",
    "        # res = center[label.flatten()]\n",
    "        # # This will flatten the label\n",
    "        # res2 = res.reshape(img.shape)\n",
    "        # # Reshape the image\n",
    "        # cv2.imshow(\"K Means\", res2)  # Display image\n",
    "        # cv2.imwrite(\"1.jpg\", res2)  # Write image onto disk\n",
    "        meanshift = cv2.pyrMeanShiftFiltering(img, sp=8, sr=16, maxLevel=1, termcrit=(cv2.TERM_CRITERIA_EPS+cv2.TERM_CRITERIA_MAX_ITER, 5, 1))\n",
    "        # Apply meanshift algorithm on to image\n",
    "        cv2.imshow(\"Output of meanshift\", meanshift)\n",
    "        # Display image\n",
    "        # cv2.imwrite(\"2.jpg\", meanshift)\n",
    "        # Write image onto disk\n",
    "        # gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        # # Convert image from RGB to GRAY\n",
    "        # ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "        # # apply thresholding to convert the image to binary\n",
    "        # fg = cv2.erode(thresh, None, iterations=1)\n",
    "        # # erode the image\n",
    "        # bgt = cv2.dilate(thresh, None, iterations=1)\n",
    "        # # Dilate the image\n",
    "        # ret, bg = cv2.threshold(bgt, 1, 128, 1)\n",
    "        # # Apply thresholding\n",
    "        # marker = cv2.add(fg, bg)\n",
    "        # # Add foreground and background\n",
    "        # canny = cv2.Canny(marker, 110, 150)\n",
    "        # # Apply canny edge detector\n",
    "        # contours, hierarchy = cv2.findContours(canny, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        # # Finding the contors in the image using chain approximation\n",
    "        # marker32 = np.int32(marker)\n",
    "        # # converting the marker to float 32 bit\n",
    "        # cv2.watershed(img,marker32)\n",
    "        # # Apply watershed algorithm\n",
    "        # m = cv2.convertScaleAbs(marker32)\n",
    "        # ret, thresh = cv2.threshold(m, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "        # # Apply thresholding on the image to convert to binary image\n",
    "        # thresh_inv = cv2.bitwise_not(thresh)\n",
    "        # # Invert the thresh\n",
    "        # res = cv2.bitwise_and(img, img, mask=thresh)\n",
    "        # # Bitwise and with the image mask thresh\n",
    "        # res3 = cv2.bitwise_and(img, img, mask=thresh_inv)\n",
    "        # # Bitwise and the image with mask as threshold invert\n",
    "        # res4 = cv2.addWeighted(res, 1, res3, 1, 0)\n",
    "        # # Take the weighted average\n",
    "        # final = cv2.drawContours(res4, contours, -1, (0, 255, 0), 1)\n",
    "        # # Draw the contours on the image with green color and pixel width is 1\n",
    "        # cv2.imshow(\"Watershed\", final)  # Display the image\n",
    "        # # cv2.imwrite(\"3.jpg\", final)  # Write the image\n",
    "\n",
    "        key = cv2.waitKey(30)\n",
    "        if key >= 0:\n",
    "            cv2.waitKey(0)\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Close the window / Release webcam\n",
    "cap.release()\n",
    "  \n",
    "# De-allocate any associated memory usage \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"raw1\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from typing import Sequence\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "\n",
    "from mmflow.apis import inference_model, init_model\n",
    "from mmflow.datasets import visualize_flow\n",
    "\n",
    "try:\n",
    "    import imageio\n",
    "except ImportError:\n",
    "    imageio = None\n",
    "\n",
    "# build the model from a config file and a checkpoint file\n",
    "model = init_model('/home/ubuntu/Workspace/mmflow/configs/pwcnet/pwcnet_ft_4x1_300k_sintel_final_384x768.py', '/home/ubuntu/Workspace/mmflow/demo/pwcnet_ft_4x1_300k_sintel_final_384x768.pth', device='cuda:0')\n",
    "# load video\n",
    "cap = cv2.VideoCapture('/home/ubuntu/Workspace/mmflow/demo/220731_170753__220731_173752_clip.mp4')\n",
    "# assert cap.isOpened(), f'Failed to load video file {args.video}'\n",
    "# get video info\n",
    "size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "        int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "i = 0\n",
    "r = []\n",
    "while (cap.isOpened()):\n",
    "    # Get frames\n",
    "    flag1, img1 = cap.read()\n",
    "    flag2, img2 = cap.read()\n",
    "\n",
    "    if not flag2:\n",
    "        break\n",
    "    \n",
    "    # i += 1\n",
    "    # if i > 10:\n",
    "    #     cap.release()\n",
    "    #     break\n",
    "    \n",
    "    result = inference_model(model, img1[100:700, :, :], img2[100:700, :, :])\n",
    "    r.append(np.sum(np.sqrt(np.power(result[:, :, 0], 2) + np.power(result[:, :, 1], 2))))\n",
    "    # np.sqrt(np.power(result[:, :, 0], 2) + np.power(result[:, :, 1], 2))\n",
    "    # print('result.shape = ', result[::0].shape)\n",
    "    # flow_map = visualize_flow(result, None)\n",
    "    # flow_map = cv2.cvtColor(flow_map, cv2.COLOR_RGB2BGR)\n",
    "    # frame = flow_map\n",
    "\n",
    "    # imgs.append(img)\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: /Users/liuziyi/Workspace/FishFeeding/pspnet_r101b-d8_512x1024_80k_cityscapes_20201226_170012-3a4d38ab.pth\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/liuziyi/Workspace/mmsegmentation')\n",
    "from mmseg.apis import inference_segmentor, init_segmentor\n",
    "import mmcv\n",
    "\n",
    "config_file = '/Users/liuziyi/Workspace/mmsegmentation/configs/pspnet/pspnet_r101b-d8_512x1024_80k_cityscapes.py'\n",
    "# config_file = '/Users/liuziyi/Workspace/FishFeeding/123.py'\n",
    "checkpoint_file = '/Users/liuziyi/Workspace/FishFeeding/pspnet_r101b-d8_512x1024_80k_cityscapes_20201226_170012-3a4d38ab.pth'\n",
    "\n",
    "# build the model from a config file and a checkpoint file\n",
    "config = mmcv.Config.fromfile(config_file)\n",
    "config[\"norm_cfg\"][\"type\"] = \"BN\"\n",
    "config[\"model\"][\"backbone\"][\"norm_cfg\"][\"type\"] = \"BN\"\n",
    "config[\"model\"][\"decode_head\"][\"norm_cfg\"][\"type\"] = \"BN\"\n",
    "config[\"model\"][\"auxiliary_head\"][\"norm_cfg\"][\"type\"] = \"BN\"\n",
    "# config[\"model\"][\"auxiliary_head\"][\"norm_cfg\"][\"type\"] = \"BN\"\n",
    "# config[\"model\"][\"auxiliary_head\"][\"norm_cfg\"][\"type\"] = \"BN\"\n",
    "\n",
    "# model = init_segmentor(config, checkpoint_file, device='cpu')\n",
    "\n",
    "# # # # test a single image and show the results\n",
    "# # # img = 'test.jpg'  # or img = mmcv.imread(img), which will only load it once\n",
    "# # # result = inference_segmentor(model, img)\n",
    "# # # # visualize the results in a new window\n",
    "# # # model.show_result(img, result, show=True)\n",
    "# # # # or save the visualization results to image files\n",
    "# # # # you can change the opacity of the painted segmentation map in (0, 1].\n",
    "# # # model.show_result(img, result, out_file='result.jpg', opacity=0.5)\n",
    "\n",
    "# # # test a video and show the results\n",
    "# # # video = mmcv.VideoReader('/home/ubuntu/Workspace/mmflow/demo/220731_170753__220731_173752_clip.mp4')\n",
    "# # # for frame in video:\n",
    "# # #    result = inference_segmentor(model, frame)\n",
    "# # #    model.show_result(frame, result, wait_time=1)\n",
    "\n",
    "# # # test a video and show the results\n",
    "model = init_segmentor(config, checkpoint_file, device='cpu')\n",
    "video = mmcv.VideoReader('/Users/liuziyi/Desktop/220731_170753__220731_173752_clip_2.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for frame in video:\n",
    "   i += 1\n",
    "   if i == 20:\n",
    "      result = inference_segmentor(model, frame)\n",
    "      model.show_result(frame, result, out_file='/Users/liuziyi/Desktop/result2.jpg', opacity=0.5)\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config (path: /Users/liuziyi/Workspace/mmsegmentation/configs/pspnet/pspnet_r101b-d8_512x1024_80k_cityscapes.py): {'norm_cfg': {'type': 'SyncBN', 'requires_grad': True}, 'model': {'type': 'EncoderDecoder', 'pretrained': 'torchvision://resnet101', 'backbone': {'type': 'ResNet', 'depth': 101, 'num_stages': 4, 'out_indices': (0, 1, 2, 3), 'dilations': (1, 1, 2, 4), 'strides': (1, 2, 1, 1), 'norm_cfg': {'type': 'SyncBN', 'requires_grad': True}, 'norm_eval': False, 'style': 'pytorch', 'contract_dilation': True}, 'decode_head': {'type': 'PSPHead', 'in_channels': 2048, 'in_index': 3, 'channels': 512, 'pool_scales': (1, 2, 3, 6), 'dropout_ratio': 0.1, 'num_classes': 19, 'norm_cfg': {'type': 'SyncBN', 'requires_grad': True}, 'align_corners': False, 'loss_decode': {'type': 'CrossEntropyLoss', 'use_sigmoid': False, 'loss_weight': 1.0}}, 'auxiliary_head': {'type': 'FCNHead', 'in_channels': 1024, 'in_index': 2, 'channels': 256, 'num_convs': 1, 'concat_input': False, 'dropout_ratio': 0.1, 'num_classes': 19, 'norm_cfg': {'type': 'SyncBN', 'requires_grad': True}, 'align_corners': False, 'loss_decode': {'type': 'CrossEntropyLoss', 'use_sigmoid': False, 'loss_weight': 0.4}}, 'train_cfg': {}, 'test_cfg': {'mode': 'whole'}}, 'dataset_type': 'CityscapesDataset', 'data_root': 'data/cityscapes/', 'img_norm_cfg': {'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}, 'crop_size': (512, 1024), 'train_pipeline': [{'type': 'LoadImageFromFile'}, {'type': 'LoadAnnotations'}, {'type': 'Resize', 'img_scale': (2048, 1024), 'ratio_range': (0.5, 2.0)}, {'type': 'RandomCrop', 'crop_size': (512, 1024), 'cat_max_ratio': 0.75}, {'type': 'RandomFlip', 'prob': 0.5}, {'type': 'PhotoMetricDistortion'}, {'type': 'Normalize', 'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}, {'type': 'Pad', 'size': (512, 1024), 'pad_val': 0, 'seg_pad_val': 255}, {'type': 'DefaultFormatBundle'}, {'type': 'Collect', 'keys': ['img', 'gt_semantic_seg']}], 'test_pipeline': [{'type': 'LoadImageFromFile'}, {'type': 'MultiScaleFlipAug', 'img_scale': (2048, 1024), 'flip': False, 'transforms': [{'type': 'Resize', 'keep_ratio': True}, {'type': 'RandomFlip'}, {'type': 'Normalize', 'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}, {'type': 'ImageToTensor', 'keys': ['img']}, {'type': 'Collect', 'keys': ['img']}]}], 'data': {'samples_per_gpu': 2, 'workers_per_gpu': 2, 'train': {'type': 'CityscapesDataset', 'data_root': 'data/cityscapes/', 'img_dir': 'leftImg8bit/train', 'ann_dir': 'gtFine/train', 'pipeline': [{'type': 'LoadImageFromFile'}, {'type': 'LoadAnnotations'}, {'type': 'Resize', 'img_scale': (2048, 1024), 'ratio_range': (0.5, 2.0)}, {'type': 'RandomCrop', 'crop_size': (512, 1024), 'cat_max_ratio': 0.75}, {'type': 'RandomFlip', 'prob': 0.5}, {'type': 'PhotoMetricDistortion'}, {'type': 'Normalize', 'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}, {'type': 'Pad', 'size': (512, 1024), 'pad_val': 0, 'seg_pad_val': 255}, {'type': 'DefaultFormatBundle'}, {'type': 'Collect', 'keys': ['img', 'gt_semantic_seg']}]}, 'val': {'type': 'CityscapesDataset', 'data_root': 'data/cityscapes/', 'img_dir': 'leftImg8bit/val', 'ann_dir': 'gtFine/val', 'pipeline': [{'type': 'LoadImageFromFile'}, {'type': 'MultiScaleFlipAug', 'img_scale': (2048, 1024), 'flip': False, 'transforms': [{'type': 'Resize', 'keep_ratio': True}, {'type': 'RandomFlip'}, {'type': 'Normalize', 'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}, {'type': 'ImageToTensor', 'keys': ['img']}, {'type': 'Collect', 'keys': ['img']}]}]}, 'test': {'type': 'CityscapesDataset', 'data_root': 'data/cityscapes/', 'img_dir': 'leftImg8bit/val', 'ann_dir': 'gtFine/val', 'pipeline': [{'type': 'LoadImageFromFile'}, {'type': 'MultiScaleFlipAug', 'img_scale': (2048, 1024), 'flip': False, 'transforms': [{'type': 'Resize', 'keep_ratio': True}, {'type': 'RandomFlip'}, {'type': 'Normalize', 'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}, {'type': 'ImageToTensor', 'keys': ['img']}, {'type': 'Collect', 'keys': ['img']}]}]}}, 'log_config': {'interval': 50, 'hooks': [{'type': 'TextLoggerHook', 'by_epoch': False}]}, 'dist_params': {'backend': 'nccl'}, 'log_level': 'INFO', 'load_from': None, 'resume_from': None, 'workflow': [('train', 1)], 'cudnn_benchmark': True, 'optimizer': {'type': 'SGD', 'lr': 0.01, 'momentum': 0.9, 'weight_decay': 0.0005}, 'optimizer_config': {}, 'lr_config': {'policy': 'poly', 'power': 0.9, 'min_lr': 0.0001, 'by_epoch': False}, 'runner': {'type': 'IterBasedRunner', 'max_iters': 80000}, 'checkpoint_config': {'by_epoch': False, 'interval': 8000}, 'evaluation': {'interval': 8000, 'metric': 'mIoU', 'pre_eval': True}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = mmcv.Config.fromfile(config_file)\n",
    "config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('openmmlab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bf0db2ea5dcb42147c811983525ae4634a0fd62145f6e37a4ba7b47b46d8e359"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
